
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiaoyu Wang &ndash; Publication</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Xiaoyu Wang</div>
<div class="menu-item"><a href=".">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publication</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="course.html">Courses</a></div>  
<div class="menu-item"><a href="software.html">Software</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Xiaoyu Wang &ndash; Publication</h1>
</div>
<h2>Preprints</h2>
<ul> 	
<li><p> Xiaoyu Wang*, Xuxing Chen*, Shiqian Ma, Tong Zhang. <br />
<a href="https://arxiv.org/abs/2410.19319"> Fully First-Order Methods for Decentralized Bilevel Optimization (under review) </a>, arXiv:2410.19319, 2024. </p>
<li><p> Ruinan Jin, Xiaoyu Wang^#, Baoxiang Wang. <br />
<a href="https://arxiv.org/abs/2409.05023"> Stability and Convergence Analysis of AdaGrad for Non-convex Optimization via Novel Stopping Time-based Techniques (under review) </a>, arXiv:2409.05023v3, 2024. </p>
<li><p> Xiaoyu Wang and Mikael Johansson. <br />
<a href="https://arxiv.org/abs/2201.10245"> On Uniform Boundedness Properties of SGD and its Momentum Variants </a>, arXiv:2201.10245, 2022. </p>
<li><p> Xiaoyu Wang and Mikael Johansson. <br />
<a href="https://arxiv.org/abs/2106.02888">Bandwidth-based Step Sizes for Non-Convex Stochastic Optimization</a>, arXiv:2106.02888, 2021.</p>
</ul>
<h2>Journal Papers</h2>
<ul>
<li><p>Xiaoyu Wang and Ya-xiang Yuan.<br />
  <a href="https://jmlr.org/papers/volume24/19-1009/19-1009.pdf">On the Convergence of Stochastic Gradient Descent with
Bandwidth-based Step Size</a><br />
  Journal of Machine Learning Research, 24(48):1–49, 2023 </p> 
<li><p>Sarit Khirirat, Xiaoyu Wang, Sindri Magnússon, and Mikael Johansson.<br />
 <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10026503">Improved Step-Size Schedules for Proximal Noisy Gradient Methods</a><br />
  IEEE Transactions on Signal Processing, 2023 </p>   
<li><p>Xiaoyu Wang and Ya-xiang Yuan.<br />
  <a href="https://arxiv.org/pdf/1904.03342.pdf">Stochastic Trust Region Methods with Trust Region Radius Depending on Probabilistic Models.</a><br />
  Journal of Computational Mathematics, 2022, 40(2): 294-334. DOI: 10.4208/jcm.2012-m2020-0144. </p>
<li><p>Xiaoyu Wang, Xiao Wang, and Ya-xiang Yuan.<br />
  <a href="https://doi.org/10.1080/10556788.2018.1471141"> Stochastic proximal quasi-Newton methods for non-convex composite optimization.</a><br />
  Optimization Methods and Software, Volume 34, Issue 5, 2019.</p>
</li> 
</ul>
<h2>Conference Papers</h2>
<ul>
<li><p> Rui Pan, Yuxing Liu, Xiaoyu Wang and Tong Zhang. <br />
<a href="https://arxiv.org/pdf/2312.14567.pdf"> Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise </a> <br />
The 24 International Conference on Learning Representations (ICLR), 2024. </p>  
<li><p>Xiaoyu Wang, Mikael Johansson and Tong Zhang.<br />
<a href="https://arxiv.org/pdf/2305.12939.pdf"> Generalized Polyak Step Size for First Order Optimization with Momentum. </a> <br />
Proceedings of the 40th International Conference on Machine Learning (ICML), 2023.</p>  
<li><p>Renjie Pi, Weizhong Zhang, Yueqi Xie, Jiahui Gao, Xiaoyu Wang, Sunghun Kim, Qifeng Chen.<br />
<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Pi_DynaFed_Tackling_Client_Data_Heterogeneity_With_Global_Dynamics_CVPR_2023_paper.pdf"> Dynafed: Tackling client data heterogeneity with global dynamics. </a> <br />
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), 2023.</p>  
<li><p> Erik Berglund,  Sarit Khirirat, and Xiaoyu Wang. <br />
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9746483">Zeroth-order random subspace Newton method. </a> <br />
accept to ICASSP IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. </p>  
<li><p>Xiaoyu Wang, Sindri Magnússon, and Mikael Johansson.<br />
<a href="https://arxiv.org/pdf/2102.09393.pdf"> On the Convergence of Step Decay Step-Size for Stochastic
Optimization. </a> <br />
35th Conference on Neural Information Processing Systems (NeurIPS 2021), 2021.</p>
<li><p>Sarit Khirirat, Xiaoyu Wang, Sindri Magnússon, Mikael Johansson.<br />
  <a href="https://ieeexplore.ieee.org/document/9414419"> Improved Step-Size Schedules for Noisy Gradient Methods. </a> <br />
  ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021.</p>
</li>
</ul>
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
